
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Summary &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'summary';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Notebooks" href="notebooks.html" />
    <link rel="prev" title="Saket Medhalavalasa" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Saket Medhalavalasa
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Summary</a></li>

<li class="toctree-l1"><a class="reference internal" href="notebooks.html">Notebooks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fsummary.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/summary.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Summary</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-summaries-for-each-step-and-experiment">Detailed Summaries for Each Step and Experiment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#links"><strong>Links:</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-create-a-normalized-database"><strong>Step 1: Create a Normalized Database</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-sql-join-and-loading-data-into-pandas"><strong>Step 2: SQL Join and Loading Data into Pandas</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-exploratory-analysis-train-test-split"><strong>Step 3: Exploratory Analysis &amp; Train/Test Split</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-data-exploration-with-yprofile-correlation-analysis"><strong>Step 4: Data Exploration with yProfile &amp; Correlation Analysis</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-1-baseline-logistic-regression"><strong>Experiment 1: Baseline Logistic Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-2-multiple-classifiers"><strong>Experiment 2: Multiple Classifiers</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-3-feature-engineering"><strong>Experiment 3: Feature Engineering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-4-feature-selection"><strong>Experiment 4: Feature Selection</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-5-pca"><strong>Experiment 5: PCA</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-6-polynomial-features-gradient-boosting"><strong>Experiment 6: Polynomial Features + Gradient Boosting</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-7-mutual-information-lightgbm"><strong>Experiment 7: Mutual Information + LightGBM</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-8-compare-experiments-pick-the-best-model"><strong>Step 8: Compare Experiments &amp; Pick the Best Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-9-deployment-serving-the-final-model"><strong>Step 9: Deployment &amp; Serving the Final Model</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<p>This is the summary of the project.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="detailed-summaries-for-each-step-and-experiment">
<h1>Detailed Summaries for Each Step and Experiment<a class="headerlink" href="#detailed-summaries-for-each-step-and-experiment" title="Link to this heading">#</a></h1>
<p>Below are <strong>detailed summaries</strong> for <strong>Steps 1–9</strong> and <strong>Experiments 1–7</strong> in the <strong>Air Quality Prediction Project</strong>. Each summary is  providing insight into the goals, methods, and outcomes of each phase.</p>
<hr class="docutils" />
<section id="links">
<h2><strong>Links:</strong><a class="headerlink" href="#links" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Streamlit App:</strong> <a href="https://airqualityapp-czxh5jumzvuy2cn5obtcag.streamlit.app/" target="_blank">Air Quality Prediction</a></p></li>
<li><p><strong>DagsHub:</strong> <a href="https://dagshub.com/saket027/EAS_503_Updated_Air_Quality_Prediction/experiments" target="_blank">Experiments</a></p></li>
<li><p><strong>DockerHub:</strong> <a href="https://hub.docker.com/repository/docker/saket027/airquality_app/general" target="_blank">Docker Image</a></p></li>
<li><p><strong>Final Deployed Model:</strong> GradientBoostingClassifier</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="step-1-create-a-normalized-database">
<h2><strong>Step 1: Create a Normalized Database</strong><a class="headerlink" href="#step-1-create-a-normalized-database" title="Link to this heading">#</a></h2>
<p>Step 1 establishes a structured foundation for the project by <strong>normalizing</strong> the raw pollution dataset into a 3NF relational schema. Rather than keeping everything in a single CSV, you split the data into logical entities—such as <code class="docutils literal notranslate"><span class="pre">Location</span></code> (for attributes like <code class="docutils literal notranslate"><span class="pre">Proximity_to_Industrial_Areas</span></code>, <code class="docutils literal notranslate"><span class="pre">Population_Density</span></code>) and <code class="docutils literal notranslate"><span class="pre">AirQuality</span></code> (for metrics like <code class="docutils literal notranslate"><span class="pre">PM2.5</span></code>, <code class="docutils literal notranslate"><span class="pre">PM10</span></code>, <code class="docutils literal notranslate"><span class="pre">NO2</span></code>, <code class="docutils literal notranslate"><span class="pre">SO2</span></code>, <code class="docutils literal notranslate"><span class="pre">CO</span></code>, and the target variable <code class="docutils literal notranslate"><span class="pre">Air</span> <span class="pre">Quality</span></code>). A link table, often called <code class="docutils literal notranslate"><span class="pre">LocationAirQuality</span></code>, matches each <code class="docutils literal notranslate"><span class="pre">Location_ID</span></code> with its corresponding <code class="docutils literal notranslate"><span class="pre">Measurement_ID</span></code>, ensuring that the relationship between geographic/demographic data and pollution metrics is clearly defined and free of redundancy.</p>
<p>You implement this using Python’s built-in <code class="docutils literal notranslate"><span class="pre">csv</span></code> library (so as not to rely on Pandas in this step) and store the tables in a <strong>SQLite</strong> database, generating a lightweight <code class="docutils literal notranslate"><span class="pre">.db</span></code> file. The approach drastically reduces data repetition; each unique location appears only once, and each unique measurement set likewise appears only once. This guards against inconsistent updates that might occur if the same location details were repeated in multiple rows. It also streamlines querying in future steps, letting you easily reassemble data with SQL joins.</p>
<p>Ultimately, <strong>Step 1</strong> transforms a single flat CSV into multiple tables that respect database normalization rules. This process paves the way for a more maintainable, extensible data setup. If you add new measurement types or new location attributes, the normalized structure accommodates these changes without bloating the dataset. By enforcing relationships strictly, you lay a solid groundwork for all subsequent data exploration and machine learning activities in the project.</p>
</section>
<hr class="docutils" />
<section id="step-2-sql-join-and-loading-data-into-pandas">
<h2><strong>Step 2: SQL Join and Loading Data into Pandas</strong><a class="headerlink" href="#step-2-sql-join-and-loading-data-into-pandas" title="Link to this heading">#</a></h2>
<p>Having stored the data in normalized tables, <strong>Step 2</strong> demonstrates how to <strong>reconstruct</strong> a single comprehensive dataset via SQL <code class="docutils literal notranslate"><span class="pre">JOIN</span></code> queries. By querying the SQLite database—where your <code class="docutils literal notranslate"><span class="pre">Location</span></code> and <code class="docutils literal notranslate"><span class="pre">AirQuality</span></code> tables are related through <code class="docutils literal notranslate"><span class="pre">LocationAirQuality</span></code>—you effectively piece together each location record with its corresponding pollution metrics in a single query result. This approach highlights the benefits of normalization: you maintain clean, integrity-safe tables internally, but can still merge them into the “flat” shape that data scientists prefer for exploration and modeling.</p>
<p>After executing the multi-table JOIN, you read the result directly into a Pandas DataFrame using <code class="docutils literal notranslate"><span class="pre">pd.read_sql_query()</span></code>. The DataFrame features key columns like <code class="docutils literal notranslate"><span class="pre">Temperature</span></code>, <code class="docutils literal notranslate"><span class="pre">PM2_5</span></code>, <code class="docutils literal notranslate"><span class="pre">Population_Density</span></code>, and so on, now merged into one table. This structure is more convenient for tasks like EDA and machine learning. You further persist this DataFrame to a new CSV file—commonly named <code class="docutils literal notranslate"><span class="pre">data_from_step_2.csv</span></code>—ensuring that subsequent steps can simply read the CSV instead of rerunning SQL queries each time.</p>
<p>In short, Step 2 seamlessly transitions from a relational model back to a data-scientist-friendly tabular format. It proves that normalization doesn’t hamper usability. Instead, it provides a strong data integrity layer while you remain free to produce consolidated DataFrames on demand. The final CSV is a critical checkpoint: if further transformations are required later, you can start from this single, consistent file, preserving the benefits of the normalized schema and ensuring reusability in every phase of your project.</p>
</section>
<hr class="docutils" />
<section id="step-3-exploratory-analysis-train-test-split">
<h2><strong>Step 3: Exploratory Analysis &amp; Train/Test Split</strong><a class="headerlink" href="#step-3-exploratory-analysis-train-test-split" title="Link to this heading">#</a></h2>
<p>Step 3 introduces <strong>exploratory data analysis</strong> and splitting your dataset into training and testing subsets. By loading <code class="docutils literal notranslate"><span class="pre">data_from_step_2.csv</span></code> into Pandas, you can readily check basic statistics and class distributions for the target variable, <code class="docutils literal notranslate"><span class="pre">Air_Quality</span></code>. Visualizing the counts of each class, such as Good, Moderate, Poor, and Hazardous, clarifies whether your dataset is imbalanced—an essential insight because it may dictate whether a stratified split is necessary.</p>
<p>If the dataset is moderately or heavily imbalanced, you apply a <strong>stratified</strong> train/test split, ensuring each class ratio in training roughly matches that of the overall dataset. Such balance is crucial in preventing your model from ignoring minor classes and inflating performance metrics artificially. You verify the distribution by examining numeric percentages or by plotting bar charts for both the train and test subsets, ensuring they closely resemble the original distribution. This step thus guards against poor generalization and inconsistent model evaluation.</p>
<p>At the same time, you may spot interesting relationships or anomalies in the dataset. Basic descriptive statistics or histograms can reveal unusual outliers (for example, extremely high PM2.5 values) or missing data patterns. Although deeper EDA (like correlation checks) arrives in later steps, Step 3 forms your initial understanding of how balanced or skewed the data is, letting you isolate a stable training portion for model development and a pristine testing subset for unbiased performance measurement. This organized structure sets the stage for fair, reliable evaluation in your subsequent modeling efforts.</p>
</section>
<hr class="docutils" />
<section id="step-4-data-exploration-with-yprofile-correlation-analysis">
<h2><strong>Step 4: Data Exploration with yProfile &amp; Correlation Analysis</strong><a class="headerlink" href="#step-4-data-exploration-with-yprofile-correlation-analysis" title="Link to this heading">#</a></h2>
<p>Step 4 elevates your exploration by utilizing <strong>ydata-profiling</strong> (formerly Pandas Profiling) to produce a comprehensive EDA report. Rather than individually calculating missing values or distribution histograms, you rely on the profiling tool’s automatic generation of summary statistics, correlations, and outlier detection. This single HTML (or Jupyter-based) report reveals everything from each column’s mean, median, and standard deviation, to a deep look at potential outliers and weird data patterns.</p>
<p>Simultaneously, you create a <strong>correlation matrix</strong>—often visualized in a Seaborn heatmap—to highlight pairs of features that are strongly or weakly correlated. High correlation could mean redundant features (e.g., PM2.5 and PM10) that might be combined or one dropped entirely. Low correlation with the target can prompt you to question whether a feature significantly contributes to predictive power. These insights from correlation checks help shape your approach to feature selection or dimensionality reduction in later experiments.</p>
<p>Additionally, the profiling tool can unearth data anomalies you might otherwise miss: non-numeric entries where numeric data is expected, columns with a surprisingly high missing rate, or distributions that are heavily skewed. Armed with this knowledge, you can correct or transform such data before feeding it into machine learning algorithms. Ultimately, Step 4 ensures you have a detailed blueprint of how your dataset behaves, guiding strategic decisions around transformations, selection, and engineering in future modeling steps.</p>
</section>
<hr class="docutils" />
<section id="experiment-1-baseline-logistic-regression">
<h2><strong>Experiment 1: Baseline Logistic Regression</strong><a class="headerlink" href="#experiment-1-baseline-logistic-regression" title="Link to this heading">#</a></h2>
<p>Experiment 1 sets the foundation by testing a <strong>Logistic Regression</strong> model as a baseline, bundled within a pipeline of basic transformations. Typically, you apply scaling (like StandardScaler), handle categorical features (OneHotEncoder), and possibly do a simple log transform to mitigate right-skewed distributions. The key objective is to see how a straightforward linear classifier fares on your dataset in terms of F1 scores (macro and micro).</p>
<p>You track performance by running cross-validation—often 10-fold—logging metrics to MLflow. The rationale behind choosing logistic regression first is twofold: (1) it’s computationally fast, offering quick feedback; (2) it’s linear, providing a minimal baseline to surpass. If you discover that logistic regression already achieves reasonably high F1 scores, you know your dataset is linearly separable to some extent. Conversely, if results are weak, you gain insight that more advanced methods (tree-based or boosting) may be needed.</p>
<p>Another crucial aspect is analyzing confusion matrices or class-based metrics. Logistic regression might excel at common classes but struggle with rarer ones, highlighting whether your data is truly balanced. By logging these results systematically in MLflow, you can compare them to subsequent experiments with more intricate transformations or classifiers. This baseline approach ensures a consistent reference point, reminding you how simple solutions measure up against more complex pipelines. If new experiments only bring marginal gains, it might indicate that your baseline was already quite competent—or that fundamental data issues remain.</p>
</section>
<hr class="docutils" />
<section id="experiment-2-multiple-classifiers">
<h2><strong>Experiment 2: Multiple Classifiers</strong><a class="headerlink" href="#experiment-2-multiple-classifiers" title="Link to this heading">#</a></h2>
<p>In Experiment 2, you expand your scope by testing a range of classifiers under a shared preprocessing pipeline. Typically, you keep the transformations from Experiment 1—like scaling, log transforms, or one-hot encoding—consistent, then swap out the final classifier step for RidgeClassifier, RandomForestClassifier, XGBClassifier, or any others you wish to explore. This approach highlights how different algorithmic families tackle your data differently, from linear models (RidgeClassifier) to ensemble trees (RandomForest, XGBoost).</p>
<p>By logging each classifier’s F1-macro and F1-micro scores in MLflow, you gain an immediate comparison. Perhaps XGBoost outperforms the linear methods, or RandomForest exhibits strong performance but is slower to train. These insights shape your hypothesis about whether your dataset’s relationships are linear, heavily non-linear, or require robust handling of outliers.</p>
<p>Another advantage is seeing how each classifier responds to the same transformations. If a classifier performs poorly, you can check if certain features or transformations are incompatible. Or you might discover that some classifiers (like tree-based ones) are less sensitive to scaling or logs, so the transformations might matter less. The experiment also guides your later steps regarding hyperparameter tuning. For instance, if random forests are best but not fully optimized, you can deepen your search in subsequent experiments. By casting a broad net in Experiment 2, you capture a wide array of potential solutions, letting you quickly zero in on the most promising approach for more focused experimentation.</p>
</section>
<hr class="docutils" />
<section id="experiment-3-feature-engineering">
<h2><strong>Experiment 3: Feature Engineering</strong><a class="headerlink" href="#experiment-3-feature-engineering" title="Link to this heading">#</a></h2>
<p>Experiment 3 tackles <strong>feature engineering</strong>—the process of creating or combining attributes to better represent patterns in the data. You might form ratios like <code class="docutils literal notranslate"><span class="pre">PM_Ratio</span> <span class="pre">=</span> <span class="pre">PM2.5</span> <span class="pre">/</span> <span class="pre">(PM10</span> <span class="pre">+</span> <span class="pre">1)</span></code>, or generate interaction terms like <code class="docutils literal notranslate"><span class="pre">Temperature</span> <span class="pre">*</span> <span class="pre">Humidity</span></code>. Each new feature attempts to highlight relationships that might be hidden when data is left in raw form. If PM2.5 and PM10 track together, building a ratio or difference might isolate a more meaningful signal.</p>
<p>In your pipeline, you insert a custom transformer that, before any scaling or encoding, calculates these new columns and appends them to the dataset. Following the transformations, you apply the same classifier steps as before—perhaps logistic regression or random forest—to see if these engineered features boost F1 scores. You log the results in MLflow, comparing them to the baseline experiments that lacked these specialized features.</p>
<p>Sometimes, you’ll see a significant lift in performance from a single well-crafted ratio or difference. Other times, multiple minor features collectively raise the score by a small margin. Or you might see no improvement if the new features add noise or replicate existing signals. Regardless, feature engineering is a potent lever, especially in tabular data tasks. By systematically testing the impact in Experiment 3, you gain clarity on which features genuinely aid classification. If results are promising, you might refine them further in subsequent experiments or combine them with different transformations, seeking that perfect synergy for higher predictive power.</p>
</section>
<hr class="docutils" />
<section id="experiment-4-feature-selection">
<h2><strong>Experiment 4: Feature Selection</strong><a class="headerlink" href="#experiment-4-feature-selection" title="Link to this heading">#</a></h2>
<p>Experiment 4 concentrates on <strong>feature selection</strong>, aiming to reduce the dimensionality of your dataset and filter out uninformative or redundant attributes. You employ multiple strategies, such as a variance threshold to drop nearly constant columns, correlation-based filtering to remove highly correlated pairs, and a model-based selection (e.g., a RandomForest’s feature importances). The objective is twofold: (1) reduce model overfitting by eliminating noisy or irrelevant features, and (2) potentially speed up training and inference.</p>
<p>You integrate these steps into a pipeline. For example, you might apply <code class="docutils literal notranslate"><span class="pre">VarianceThreshold</span></code> first, then a custom correlation filter, and finally a <code class="docutils literal notranslate"><span class="pre">SelectFromModel</span></code> stage that uses a random forest to rank feature importance. After this selection, the pruned set of features moves on to a classifier (like logistic regression). By logging F1 scores, you see if you can improve or at least maintain performance with fewer features. Sometimes, removing extraneous variables clarifies the signal for the model, leading to higher accuracy.</p>
<p>Experiment 4 clarifies how your data’s complexity relates to your model’s generalization capabilities. If performance increases or stays the same with fewer features, that’s a win, as it means your solution is simpler and less prone to overfit. If performance drops, it may suggest that certain features presumed unimportant were actually valuable. The synergy between correlation checks, variance thresholds, and model-based importance forms a robust combination for trimming down your feature set. Ultimately, you can confirm that streamlining attributes can be just as pivotal as adding new ones.</p>
</section>
<hr class="docutils" />
<section id="experiment-5-pca">
<h2><strong>Experiment 5: PCA</strong><a class="headerlink" href="#experiment-5-pca" title="Link to this heading">#</a></h2>
<p>In Experiment 5, you investigate <strong>Principal Component Analysis (PCA)</strong> to see whether dimensionality reduction helps your model. PCA identifies orthogonal axes (principal components) that capture the greatest variance in the data. You typically run it after basic transformations—like scaling or log transforms—so the principal components are derived from consistent numeric distributions. A scree plot visualizes how each component’s explained variance tapers off, letting you choose a cutoff that captures, say, 95% of the variance.</p>
<p>By training a classifier (maybe logistic regression) on just these selected principal components, you measure if performance improves or remains stable. Sometimes, reducing dimensions can remove noise or correlated features, leading to a simpler model that generalizes better. In other cases, you might see performance dip slightly, since PCA is purely unsupervised and may discard class-related variance. However, even a small drop in F1 might be worth it if the model speeds up significantly or if fewer features means fewer resources in production.</p>
<p>You log each run in MLflow, comparing F1 scores before and after PCA. Additionally, you observe how many components were chosen (<code class="docutils literal notranslate"><span class="pre">n_components</span></code>). This approach fosters a numeric approach to dimensionality decisions—rather than guesswork. If you find that 10 components suffice to match prior performance, you might adopt that as your new baseline. If the best explained variance threshold leads to only modest changes, you can weigh the trade-offs in training speed or interpretability. Overall, Experiment 5 clarifies whether PCA-driven dimensionality reduction is beneficial for your specific air quality dataset.</p>
</section>
<hr class="docutils" />
<section id="experiment-6-polynomial-features-gradient-boosting">
<h2><strong>Experiment 6: Polynomial Features + Gradient Boosting</strong><a class="headerlink" href="#experiment-6-polynomial-features-gradient-boosting" title="Link to this heading">#</a></h2>
<p>Experiment 6 tests a more advanced, <strong>custom approach</strong>. By adding polynomial features in your pipeline, you capture non-linear interactions among numeric variables—like <code class="docutils literal notranslate"><span class="pre">(PM2.5)^2</span></code>, <code class="docutils literal notranslate"><span class="pre">Temperature</span> <span class="pre">*</span> <span class="pre">Humidity</span></code>, or <code class="docutils literal notranslate"><span class="pre">(CO)^2</span></code>. Such expansions can uncover complex patterns that linear transformations miss. Then, you pair these polynomial features with a <strong>GradientBoostingClassifier</strong>, a powerful ensemble method known for boosting performance on structured data.</p>
<p>The hypothesis is that the synergy of polynomial expansions plus gradient boosting could yield higher F1 scores than earlier experiments. After integrating these steps—scaling, polynomial expansions, and gradient boosting—into a pipeline, you log cross-validation results in MLflow. The new features might significantly improve model capacity to separate classes, especially if your original dataset had non-linear relationships that simpler transformations didn’t exploit.</p>
<p>You watch the F1-macro and F1-micro means to see if polynomial expansions add genuine value. Sometimes, they do wonders for non-linear classification tasks; other times, they add too many features and risk overfitting. But because you store the entire pipeline as an artifact, if the experiment shows a strong boost in performance, it becomes a prime candidate for final selection. Additionally, you may tune parameters in the GradientBoostingClassifier—like number of estimators or learning rate—to refine results. Experiment 6’s success typically signals that polynomial interactions, combined with an advanced ensemble method, can lead to robust predictions in your air quality scenario.</p>
</section>
<hr class="docutils" />
<section id="experiment-7-mutual-information-lightgbm">
<h2><strong>Experiment 7: Mutual Information + LightGBM</strong><a class="headerlink" href="#experiment-7-mutual-information-lightgbm" title="Link to this heading">#</a></h2>
<p>Experiment 7 tries <strong>another custom approach</strong> using mutual information-based feature selection plus <strong>LightGBM</strong>. Mutual information evaluates how much knowing a given feature reduces uncertainty about the target class, thus highlighting the top K most relevant features. After this selection step, you pass the reduced set of features to a <code class="docutils literal notranslate"><span class="pre">LGBMClassifier</span></code>—a gradient boosting library often praised for fast performance and strong results.</p>
<p>Because LightGBM can handle large data efficiently and incorporate built-in handling for various feature types, it pairs nicely with a strategic feature selection approach. By dropping features that provide minimal mutual information, you streamline the model’s training to only the most informative variables—like certain pollution metrics strongly tied to the target. You then measure if the synergy of mutual information and LightGBM yields higher F1 scores relative to previous experiments.</p>
<p>In typical MLflow logging fashion, you compare F1-macro and F1-micro. If results jump significantly, it suggests your dataset had extraneous columns that overshadowed key signals, and that LightGBM’s fast boosting approach capitalized on the refined feature set. On the other hand, if performance remains similar, it might indicate that LightGBM could handle those extraneous columns anyway, or that your top K features alone weren’t enough. Nonetheless, this experiment demonstrates a more targeted feature selection method, pivoting from purely correlation-based or random forest feature importance to a direct measure of mutual information with the target. If LightGBM with selected features stands out, it can become a prime contender for final model selection.</p>
</section>
<hr class="docutils" />
<section id="step-8-compare-experiments-pick-the-best-model">
<h2><strong>Step 8: Compare Experiments &amp; Pick the Best Model</strong><a class="headerlink" href="#step-8-compare-experiments-pick-the-best-model" title="Link to this heading">#</a></h2>
<p>Step 8 unifies all prior experiments by querying MLflow for each experiment’s best run. Typically, you define “best” by the highest <code class="docutils literal notranslate"><span class="pre">f1_macro_mean</span></code> or another chosen metric. You compile these top runs into a table or bar chart, letting you visually compare, for instance, how polynomial expansions with gradient boosting (Experiment 6) stacks up against mutual information + LightGBM (Experiment 7). Once you identify the single best run overall—perhaps a run from Experiment 6 with a certain pipeline configuration—you load that run’s model from MLflow, ensuring no guesswork.</p>
<p>You finalize your choice by <strong>saving</strong> the loaded pipeline as <code class="docutils literal notranslate"><span class="pre">final_model.joblib</span></code> locally, bridging ephemeral experimentation and permanent deployment. This step underscores the advantage of tracking everything in MLflow: you have a reproducible record of which transformations and hyperparameters led to the best result. If there’s a tie, you might weigh training speed, interpretability, or confusion matrix specifics for certain classes.</p>
<p>Ultimately, Step 8 cements the entire modeling phase, producing a single artifact that stands above the rest in performance. By enumerating each experiment’s champion run, you confirm the methodical nature of your experimentation—no single approach was overlooked or lost. This fosters confidence that the selected model indeed harnesses the best combination of transformations, feature engineering, and classifier choice. Step 8 thus readies you for final deployment steps, ensuring you rely on the truly best pipeline discovered among all your trials.</p>
</section>
<hr class="docutils" />
<section id="step-9-deployment-serving-the-final-model">
<h2><strong>Step 9: Deployment &amp; Serving the Final Model</strong><a class="headerlink" href="#step-9-deployment-serving-the-final-model" title="Link to this heading">#</a></h2>
<p>Step 9 completes the journey by <strong>deploying</strong> your chosen best model as a production-ready service. You begin by writing a FastAPI application (<code class="docutils literal notranslate"><span class="pre">main.py</span></code>) that loads your locally saved <code class="docutils literal notranslate"><span class="pre">final_model.joblib</span></code>. The app defines a <code class="docutils literal notranslate"><span class="pre">/predict</span></code> endpoint, which expects JSON data matching your feature schema. When a request arrives, the code transforms the input into a DataFrame, calls <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code>, and returns the predicted class label (e.g., Good, Moderate, Poor, Hazardous). This effectively turns your pipeline—complete with scaling, encoding, or advanced transformations—into a REST API anyone can query.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Saket Medhalavalasa</p>
      </div>
    </a>
    <a class="right-next"
       href="notebooks.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Notebooks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-summaries-for-each-step-and-experiment">Detailed Summaries for Each Step and Experiment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#links"><strong>Links:</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-create-a-normalized-database"><strong>Step 1: Create a Normalized Database</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-sql-join-and-loading-data-into-pandas"><strong>Step 2: SQL Join and Loading Data into Pandas</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-exploratory-analysis-train-test-split"><strong>Step 3: Exploratory Analysis &amp; Train/Test Split</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-data-exploration-with-yprofile-correlation-analysis"><strong>Step 4: Data Exploration with yProfile &amp; Correlation Analysis</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-1-baseline-logistic-regression"><strong>Experiment 1: Baseline Logistic Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-2-multiple-classifiers"><strong>Experiment 2: Multiple Classifiers</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-3-feature-engineering"><strong>Experiment 3: Feature Engineering</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-4-feature-selection"><strong>Experiment 4: Feature Selection</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-5-pca"><strong>Experiment 5: PCA</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-6-polynomial-features-gradient-boosting"><strong>Experiment 6: Polynomial Features + Gradient Boosting</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-7-mutual-information-lightgbm"><strong>Experiment 7: Mutual Information + LightGBM</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-8-compare-experiments-pick-the-best-model"><strong>Step 8: Compare Experiments &amp; Pick the Best Model</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-9-deployment-serving-the-final-model"><strong>Step 9: Deployment &amp; Serving the Final Model</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>